{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp9qX1cFWVMN"
      },
      "source": [
        "# ECE180 HW2\n",
        "\n",
        "## Name:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIkpwMeKWDLV"
      },
      "source": [
        "# k-Nearest Neighbor (kNN) exercise\n",
        "\n",
        "*Complete and hand in this completed worksheet (including its outputs) with your assignment submission.\n",
        "\n",
        "The kNN classifier consists of two stages:\n",
        "\n",
        "- During training, the classifier takes the training data and simply remembers it\n",
        "- During testing, kNN classifies every test image by comparing to all training images and transfering the labels of the k most similar training examples\n",
        "- The value of k is cross-validated\n",
        "\n",
        "In this exercise you will implement these steps and understand the basic Image Classification pipeline, cross-validation, and gain proficiency in writing efficient, vectorized code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "X4f0BvrwWDLW"
      },
      "outputs": [],
      "source": [
        "# Run some setup code for this notebook.\n",
        "\n",
        "from builtins import range\n",
        "from builtins import object\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
        "# rather than in a new window.\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1BqheuvWDLX"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import cifar10\n",
        "# load dataset\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "y_train = y_train.squeeze()\n",
        "y_test = y_test.squeeze()\n",
        "\n",
        "# As a sanity check, we print out the size of the training and test data.\n",
        "print('Training data shape: ', X_train.shape)\n",
        "print('Training labels shape: ', y_train.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amPtRqM-WDLY"
      },
      "outputs": [],
      "source": [
        "# Visualize some examples from the dataset.\n",
        "# We show a few examples of training images from each class.\n",
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "num_classes = len(classes)\n",
        "samples_per_class = 7\n",
        "for y, cls in enumerate(classes):\n",
        "    idxs = np.flatnonzero(y_train == y)\n",
        "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
        "    for i, idx in enumerate(idxs):\n",
        "        plt_idx = i * num_classes + y + 1\n",
        "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
        "        plt.imshow(X_train[idx].astype('uint8'))\n",
        "        plt.axis('off')\n",
        "        if i == 0:\n",
        "            plt.title(cls)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pnyVzte3WDLY"
      },
      "outputs": [],
      "source": [
        "# Subsample the data for more efficient code execution in this exercise\n",
        "# Take 5,000 samples from training set and 500 samples from test set.\n",
        "\n",
        "num_training = 5000\n",
        "num_test = 500\n",
        "\n",
        "#####################################################################\n",
        "# TODO:                                                             #\n",
        "# Randomly select 5000 and 500 samples from train and test set.     #\n",
        "# Overwrite them to X_train, y_train, X_test, y_test.               #\n",
        "# Use the above visualization to double-check images and labels.    #\n",
        "#####################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "indices_train = np.random.choice(X_train.shape[0], num_training, replace=False)\n",
        "X_train = X_train[indices_train]\n",
        "y_train = y_train[indices_train]\n",
        "\n",
        "indices_test = np.random.choice(X_test.shape[0], num_test, replace=False)\n",
        "X_test = X_test[indices_test]\n",
        "y_test = y_test[indices_test]\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSVEUYLQWDLY"
      },
      "outputs": [],
      "source": [
        "# Reshape the image data into rows\n",
        "# Output should be (5000, 3072) (500, 3072)\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
        "print(X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJwzRcmebMns"
      },
      "outputs": [],
      "source": [
        "class KNearestNeighbor(object):\n",
        "    \"\"\" a kNN classifier \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def train(self, X, y):\n",
        "        \"\"\"\n",
        "        Train the classifier. For k-nearest neighbors this is just\n",
        "        memorizing the training data.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (num_train, D) containing the training data\n",
        "          consisting of num_train samples each of dimension D.\n",
        "        - y: A numpy array of shape (N,) containing the training labels, where\n",
        "             y[i] is the label for X[i].\n",
        "        \"\"\"\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def predict(self, X, k=1):\n",
        "        \"\"\"\n",
        "        Predict labels for test data using this classifier.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (num_test, D) containing test data consisting\n",
        "             of num_test samples each of dimension D.\n",
        "        - k: The number of nearest neighbors that vote for the predicted labels.\n",
        "\n",
        "        Returns:\n",
        "        - y: A numpy array of shape (num_test,) containing predicted labels for the\n",
        "          test data, where y[i] is the predicted label for the test point X[i].\n",
        "        \"\"\"\n",
        "        dists = self.compute_distances(X)\n",
        "        return self.predict_labels(dists, k=k)\n",
        "\n",
        "    def compute_distances(self, X):\n",
        "        \"\"\"\n",
        "        Compute the distance between each test point in X and each training point\n",
        "        in self.X_train\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (num_test, D) containing test data.\n",
        "\n",
        "        Returns:\n",
        "        - dists: A numpy array of shape (num_test, num_train) where dists[i, j]\n",
        "          is the distance between the ith test point and the jth training\n",
        "          point.\n",
        "        \"\"\"\n",
        "        num_test = X.shape[0]\n",
        "        num_train = self.X_train.shape[0]\n",
        "        dists = np.zeros((num_test, num_train))\n",
        "        #####################################################################\n",
        "        # TODO:                                                             #\n",
        "        # Compute the l2 distance between the ith test point and the jth    #\n",
        "        # training point, and store the result in dists[i, j]. You should   #\n",
        "        # not use a loop over dimension, nor use np.linalg.norm().          #\n",
        "        # Try to use numpy function to compute dists by a single function   #\n",
        "        # call, comparing a matrix and a matrix. (deduction 0)              #\n",
        "        # You can iterate for test samples to compute distances between a   #\n",
        "        # vector (a test sample) and a matrix (train samples). (deduction 2)#\n",
        "        # Or you can have two iterations (two for) to compute distances     #\n",
        "        # between a vector (a test sample) and a vector (a train sample),   #\n",
        "        # but that will be slow and your cross-validation will take a long  #\n",
        "        # time.                                                             #\n",
        "        #####################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        \n",
        "        for i in range(num_test):\n",
        "          dists[i] = np.sqrt(np.sum(np.square(X[i] - self.X_train), axis = 1))\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        return dists\n",
        "\n",
        "\n",
        "    def predict_labels(self, dists, k=1):\n",
        "        \"\"\"\n",
        "        Given a matrix of distances between test points and training points,\n",
        "        predict a label for each test point.\n",
        "\n",
        "        Inputs:\n",
        "        - dists: A numpy array of shape (num_test, num_train) where dists[i, j]\n",
        "          gives the distance betwen the ith test point and the jth training point.\n",
        "\n",
        "        Returns:\n",
        "        - y: A numpy array of shape (num_test,) containing predicted labels for the\n",
        "          test data, where y[i] is the predicted label for the test point X[i].\n",
        "        \"\"\"\n",
        "        num_test = dists.shape[0]\n",
        "        y_pred = np.zeros(num_test, dtype=int)  # Ensure dtype is int for indexing\n",
        "        for i in range(num_test):\n",
        "            # A list of length k storing the labels of the k nearest neighbors to\n",
        "            # the ith test point.\n",
        "            #########################################################################\n",
        "            # TODO:                                                                 #\n",
        "            # Use the distance matrix to find the k nearest neighbors of the ith    #\n",
        "            # testing point, and use self.y_train to find the labels of these       #\n",
        "            # neighbors. Store these labels in closest_y.                           #\n",
        "            # Hint: Look up the function numpy.argsort.                             #\n",
        "            #########################################################################\n",
        "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "            \n",
        "            closest_y_indices = np.argsort(dists[i])[:k]\n",
        "            closest_y = self.y_train[closest_y_indices]\n",
        "\n",
        "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "            #########################################################################\n",
        "            # TODO:                                                                 #\n",
        "            # Now that you have found the labels of the k nearest neighbors, you    #\n",
        "            # need to find the most common label in the list closest_y of labels.   #\n",
        "            # Store this label in y_pred[i]. Break ties by choosing the smaller     #\n",
        "            # label.                                                                #\n",
        "            #########################################################################\n",
        "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "            # Voting mechanism: Find the most frequent label\n",
        "            \n",
        "            unique, counts = np.unique(closest_y, return_counts=True)\n",
        "            y_pred[i] = unique[np.argmax(counts)]\n",
        "            \n",
        "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZxsNYKZbrOJ"
      },
      "outputs": [],
      "source": [
        "# Create a kNN classifier instance.\n",
        "# Remember that training a kNN classifier is a noop:\n",
        "# the Classifier simply remembers the data and does no further processing\n",
        "classifier = KNearestNeighbor()\n",
        "classifier.train(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gE92Rv0dWDLZ"
      },
      "outputs": [],
      "source": [
        "# Now implement the function predict_labels and run the code below:\n",
        "# We use k = 1 (which is Nearest Neighbor).\n",
        "dists = classifier.compute_distances(X_test)\n",
        "y_test_pred = classifier.predict_labels(dists, k=1)\n",
        "\n",
        "# Compute and print the fraction of correctly predicted examples\n",
        "num_correct = np.sum(y_test_pred == y_test)\n",
        "accuracy = float(num_correct) / num_test\n",
        "print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqjPCQYeWDLZ"
      },
      "source": [
        "You should expect to see approximately `20%` accuracy. Now lets try out a larger `k`, say `k = 5`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCYj38TwWDLZ"
      },
      "outputs": [],
      "source": [
        "y_test_pred = classifier.predict_labels(dists, k=5)\n",
        "num_correct = np.sum(y_test_pred == y_test)\n",
        "accuracy = float(num_correct) / num_test\n",
        "print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD2CakbtWDLa"
      },
      "source": [
        "### Cross-validation\n",
        "\n",
        "We have implemented the k-Nearest Neighbor classifier but we set the value k = 5 arbitrarily. We will now determine the best value of this hyperparameter with cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCib9hzTWDLa"
      },
      "outputs": [],
      "source": [
        "num_folds = 5\n",
        "k_choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100] #You can reduce the k's if the running time is too long.\n",
        "# k_choices = [1, 3, 5, 8, 10, 12]\n",
        "\n",
        "X_train_folds = []\n",
        "y_train_folds = []\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Split up the training data into folds. After splitting, X_train_folds and    #\n",
        "# y_train_folds should each be lists of length num_folds, where                #\n",
        "# y_train_folds[i] is the label vector for the points in X_train_folds[i].     #\n",
        "# Hint: Look up the numpy array_split function.                                #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "X_train_folds = np.array_split(X_train, num_folds)\n",
        "y_train_folds = np.array_split(y_train, num_folds)\n",
        "\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "# A dictionary holding the accuracies for different values of k that we find\n",
        "# when running cross-validation. After running cross-validation,\n",
        "# k_to_accuracies[k] should be a list of length num_folds giving the different\n",
        "# accuracy values that we found when using that value of k.\n",
        "k_to_accuracies = {}\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Perform k-fold cross validation to find the best value of k. For each        #\n",
        "# possible value of k, run the k-nearest-neighbor algorithm num_folds times,   #\n",
        "# where in each case you use all but one of the folds as training data and the #\n",
        "# last fold as a validation set. Store the accuracies for all fold and all     #\n",
        "# values of k in the k_to_accuracies dictionary.                               #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "for curr_k in k_choices:\n",
        "    k_to_accuracies[curr_k] = []\n",
        "    for curr_fold in range(num_folds):\n",
        "        classifier = KNearestNeighbor()\n",
        "        classifier.train(np.concatenate(X_train_folds[:curr_fold] + X_train_folds[curr_fold + 1:]), np.concatenate(y_train_folds[:curr_fold] + y_train_folds[curr_fold + 1:]))\n",
        "        y_pred_fold = classifier.predict(X_train_folds[curr_fold], k = curr_k)\n",
        "        k_to_accuracies[curr_k].append(np.mean(y_pred_fold == y_train_folds[curr_fold]))\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "# Print out the computed accuracies\n",
        "for k in sorted(k_to_accuracies):\n",
        "    for accuracy in k_to_accuracies[k]:\n",
        "        print('k = %d, accuracy = %f' % (k, accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwHDQgyMWDLa"
      },
      "outputs": [],
      "source": [
        "# plot the raw observations\n",
        "for k in k_choices:\n",
        "    accuracies = k_to_accuracies[k]\n",
        "    plt.scatter([k] * len(accuracies), accuracies)\n",
        "\n",
        "# plot the trend line with error bars that correspond to standard deviation\n",
        "accuracies_mean = np.array([np.mean(v) for k,v in sorted(k_to_accuracies.items())])\n",
        "accuracies_std = np.array([np.std(v) for k,v in sorted(k_to_accuracies.items())])\n",
        "plt.errorbar(k_choices, accuracies_mean, yerr=accuracies_std)\n",
        "plt.title('Cross-validation on k')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Cross-validation accuracy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EywaTQx_WDLa"
      },
      "outputs": [],
      "source": [
        "# Based on the cross-validation results above, choose the best value for k,\n",
        "# retrain the classifier using all the training data, and test it on the test\n",
        "# data. You should be able to get above 28% accuracy on the test data.\n",
        "best_k = k_choices[accuracies_mean.argmax()]\n",
        "\n",
        "classifier = KNearestNeighbor()\n",
        "classifier.train(X_train, y_train)\n",
        "y_test_pred = classifier.predict(X_test, k=best_k)\n",
        "\n",
        "# Compute and display the accuracy\n",
        "num_correct = np.sum(y_test_pred == y_test)\n",
        "accuracy = float(num_correct) / num_test\n",
        "print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_RAMZjbWDLd"
      },
      "source": [
        "# Softmax exercise\n",
        "\n",
        "\n",
        "This exercise is analogous to the SVM exercise. You will:\n",
        "\n",
        "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
        "- implement the fully-vectorized expression for its **analytic gradient**\n",
        "- **check your implementation** with numerical gradient\n",
        "- use a validation set to **tune the learning rate and regularization** strength\n",
        "- **optimize** the loss function with **SGD**\n",
        "- **visualize** the final learned weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-krIoxqWDLd"
      },
      "outputs": [],
      "source": [
        "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
        "    \"\"\"\n",
        "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
        "    it for the linear classifier. These are the same steps as we used for the\n",
        "    SVM, but condensed to a single function.\n",
        "    \"\"\"\n",
        "    # Load the raw CIFAR-10 data\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "    y_train = y_train.squeeze()\n",
        "    y_test = y_test.squeeze()\n",
        "\n",
        "    # subsample the data\n",
        "    mask = list(range(num_training, num_training + num_validation))\n",
        "    X_val = X_train[mask]\n",
        "    y_val = y_train[mask]\n",
        "    mask = list(range(num_training))\n",
        "    X_train = X_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    mask = list(range(num_test))\n",
        "    X_test = X_test[mask]\n",
        "    y_test = y_test[mask]\n",
        "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
        "    X_dev = X_train[mask]\n",
        "    y_dev = y_train[mask]\n",
        "\n",
        "    # Preprocessing: reshape the image data into rows\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
        "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
        "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
        "\n",
        "    # Normalize the data: subtract the mean image\n",
        "    mean_image_original = np.mean(X_train, axis = 0)\n",
        "    mean_image = mean_image_original.astype(np.uint8)\n",
        "\n",
        "    X_train -= mean_image\n",
        "    X_val -= mean_image\n",
        "    X_test -= mean_image\n",
        "    X_dev -= mean_image\n",
        "\n",
        "    # add bias dimension and transform into columns\n",
        "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
        "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
        "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
        "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
        "\n",
        "\n",
        "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
        "try:\n",
        "   del X_train, y_train\n",
        "   del X_test, y_test\n",
        "   print('Clear previously loaded data.')\n",
        "except:\n",
        "   pass\n",
        "\n",
        "# Invoke the above function to get our data.\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
        "\n",
        "print('Train data shape: ', X_train.shape)\n",
        "print('Train labels shape: ', y_train.shape)\n",
        "print('Validation data shape: ', X_val.shape)\n",
        "print('Validation labels shape: ', y_val.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)\n",
        "print('dev data shape: ', X_dev.shape)\n",
        "print('dev labels shape: ', y_dev.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDxjvorDWDLd"
      },
      "source": [
        "## Softmax Classifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYxVmypehOG2"
      },
      "outputs": [],
      "source": [
        "def softmax_loss(W, X, y, reg):\n",
        "    \"\"\"\n",
        "    Softmax loss function, vectorized version.\n",
        "\n",
        "    Inputs and outputs are the same as softmax_loss_naive.\n",
        "    \"\"\"\n",
        "    # Initialize the loss and gradient to zero.\n",
        "    loss = 0.0\n",
        "    dW = np.zeros_like(W)\n",
        "\n",
        "    #############################################################################\n",
        "    # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
        "    # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
        "    # here, it is easy to run into numeric instability. Don't forget the        #\n",
        "    # regularization!                                                           #\n",
        "    #############################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    return loss, dW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJMAA4iUjC2R"
      },
      "outputs": [],
      "source": [
        "class Softmax(object):\n",
        "    def __init__(self):\n",
        "        self.W = None\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        X,\n",
        "        y,\n",
        "        learning_rate=1e-3,\n",
        "        reg=1e-5,\n",
        "        num_iters=100,\n",
        "        batch_size=200,\n",
        "        verbose=False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Train this linear classifier using stochastic gradient descent.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "          training samples each of dimension D.\n",
        "        - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
        "          means that X[i] has label 0 <= c < C for C classes.\n",
        "        - learning_rate: (float) learning rate for optimization.\n",
        "        - reg: (float) regularization strength.\n",
        "        - num_iters: (integer) number of steps to take when optimizing\n",
        "        - batch_size: (integer) number of training examples to use at each step.\n",
        "        - verbose: (boolean) If true, print progress during optimization.\n",
        "\n",
        "        Outputs:\n",
        "        A list containing the value of the loss function at each training iteration.\n",
        "        \"\"\"\n",
        "        num_train, dim = X.shape\n",
        "        num_classes = (\n",
        "            np.max(y) + 1\n",
        "        )  # assume y takes values 0...K-1 where K is number of classes\n",
        "        if self.W is None:\n",
        "            # lazily initialize W\n",
        "            self.W = 0.001 * np.random.randn(dim, num_classes)\n",
        "\n",
        "        # Run stochastic gradient descent to optimize W\n",
        "        loss_history = []\n",
        "        for it in range(num_iters):\n",
        "            X_batch = None\n",
        "            y_batch = None\n",
        "\n",
        "            indices = np.random.choice(num_train, batch_size)\n",
        "            X_batch = X[indices]\n",
        "            y_batch = y[indices]\n",
        "\n",
        "            # evaluate loss and gradient\n",
        "            loss, grad = self.loss(X_batch, y_batch, reg)\n",
        "            loss_history.append(loss)\n",
        "\n",
        "            self.W -= learning_rate * grad\n",
        "\n",
        "            if verbose and it % 100 == 0:\n",
        "                print(\"iteration %d / %d: loss %f\" % (it, num_iters, loss))\n",
        "\n",
        "        return loss_history\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Use the trained weights of this linear classifier to predict labels for\n",
        "        data points.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "          training samples each of dimension D.\n",
        "\n",
        "        Returns:\n",
        "        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
        "          array of length N, and each element is an integer giving the predicted\n",
        "          class.\n",
        "        \"\"\"\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        return y_pred\n",
        "\n",
        "    def loss(self, X_batch, y_batch, reg):\n",
        "        \"\"\"\n",
        "        Compute the loss function and its derivative.\n",
        "        Subclasses will override this.\n",
        "\n",
        "        Inputs:\n",
        "        - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n",
        "          data points; each point has dimension D.\n",
        "        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
        "        - reg: (float) regularization strength.\n",
        "\n",
        "        Returns: A tuple containing:\n",
        "        - loss as a single float\n",
        "        - gradient with respect to self.W; an array of the same shape as W\n",
        "        \"\"\"\n",
        "        return softmax_loss(self.W, X_batch, y_batch, reg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX5Qwo6GWDLc"
      },
      "source": [
        "### Stochastic Gradient Descent\n",
        "\n",
        "We now have vectorized and efficient expressions for the loss, the gradient and our gradient matches the numerical gradient. We are therefore ready to do SGD to minimize the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r98DzSN6WDLc"
      },
      "outputs": [],
      "source": [
        "# In the file linear_classifier.py, implement SGD in the function\n",
        "softmax = Softmax()\n",
        "loss_hist = softmax.train(X_train, y_train, learning_rate=1e-7, reg=2.5e4,\n",
        "                      num_iters=1500, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqzJziYNWDLc"
      },
      "outputs": [],
      "source": [
        "# Write the Softmax.predict function and evaluate the performance on both the\n",
        "# training and validation set\n",
        "y_train_pred = softmax.predict(X_train)\n",
        "print('training accuracy: %f' % (np.mean(y_train == y_train_pred), ))\n",
        "y_val_pred = softmax.predict(X_val)\n",
        "print('validation accuracy: %f' % (np.mean(y_val == y_val_pred), ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YGCP9prWDLc"
      },
      "outputs": [],
      "source": [
        "# A useful debugging strategy is to plot the loss as a function of\n",
        "# iteration number:\n",
        "plt.plot(loss_hist)\n",
        "plt.xlabel('Iteration number')\n",
        "plt.ylabel('Loss value')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGCG8vK-WDLe"
      },
      "outputs": [],
      "source": [
        "# Use the validation set to tune hyperparameters (regularization strength and\n",
        "# learning rate). You should experiment with different ranges for the learning\n",
        "# rates and regularization strengths; if you are careful you should be able to\n",
        "# get a classification accuracy of over 0.35 on the validation set.\n",
        "results = {}\n",
        "best_val = -1\n",
        "best_softmax = None\n",
        "learning_rates = [1e-7, 2e-6, 2.5e-6]\n",
        "regularization_strengths = [1e3, 1e4, 2e4, 2.5e4, 3e4, 3.5e4]\n",
        "\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Use the validation set to set the learning rate and regularization strength. #\n",
        "# This should be identical to the validation that you did for the SVM; save    #\n",
        "# the best trained softmax classifer in best_softmax.                          #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#                              END OF YOUR CODE                                #\n",
        "################################################################################\n",
        "\n",
        "# Print out results.\n",
        "for lr, reg in sorted(results):\n",
        "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
        "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
        "                lr, reg, train_accuracy, val_accuracy))\n",
        "\n",
        "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Qt6XucgWDLd"
      },
      "outputs": [],
      "source": [
        "# Visualize the cross-validation results\n",
        "import math\n",
        "x_scatter = [math.log10(x[0]) for x in results]\n",
        "y_scatter = [math.log10(x[1]) for x in results]\n",
        "\n",
        "# plot training accuracy\n",
        "marker_size = 100\n",
        "colors = [results[x][0] for x in results]\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.scatter(x_scatter, y_scatter, marker_size, c=colors)\n",
        "plt.colorbar()\n",
        "plt.xlabel('log learning rate')\n",
        "plt.ylabel('log regularization strength')\n",
        "plt.title('CIFAR-10 training accuracy')\n",
        "\n",
        "# plot validation accuracy\n",
        "colors = [results[x][1] for x in results] # default size of markers is 20\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.scatter(x_scatter, y_scatter, marker_size, c=colors)\n",
        "plt.colorbar()\n",
        "plt.xlabel('log learning rate')\n",
        "plt.ylabel('log regularization strength')\n",
        "plt.title('CIFAR-10 validation accuracy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icqBNqR3WDLe"
      },
      "outputs": [],
      "source": [
        "# evaluate on test set\n",
        "# Evaluate the best softmax on test set\n",
        "y_test_pred = best_softmax.predict(X_test)\n",
        "test_accuracy = np.mean(y_test == y_test_pred)\n",
        "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Putwp_VGWDLe"
      },
      "outputs": [],
      "source": [
        "# Visualize the learned weights for each class\n",
        "w = best_softmax.W[:-1,:] # strip out the bias\n",
        "w = w.reshape(32, 32, 3, 10)\n",
        "\n",
        "w_min, w_max = np.min(w), np.max(w)\n",
        "\n",
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "\n",
        "    # Rescale the weights to be between 0 and 255\n",
        "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
        "    plt.imshow(wimg.astype('uint8'))\n",
        "    plt.axis('off')\n",
        "    plt.title(classes[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZOiKYj9_Oj5"
      },
      "source": [
        "# Submission\n",
        "\n",
        "It should be a pdf file.\n",
        "\n",
        "Please use the print function of your browser (Chrome, etc.) to convert this notebook to PDF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpSVmDZw-qnM"
      },
      "source": [
        "Content is mostly borrowed from Stanford CS231n."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
